{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query News Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import dill\n",
    "import requests\n",
    "import urllib\n",
    "import bs4\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from kss import split_sentences\n",
    "\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm, trange\n",
    "from crawler.utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. QueryNewsCrawler class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 line by line coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"코스피\"\n",
    "query = urllib.parse.quote(query)\n",
    "\n",
    "main_url = \"https://search.naver.com/search.naver\"\n",
    "query_url = \"?where=news&sm=tab_pge&query=\"\n",
    "# 네이버 뉴스의 경우 page가 아닌\n",
    "# 뉴스 기사 개수 카운팅으로 보여주고 있음\n",
    "# 1 ~ 4000개까지 10개 단위로 보여줌\n",
    "page_url = \"&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=23&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=\"\n",
    "\n",
    "start = 1\n",
    "start_range = list(range(1, 4000, 10))\n",
    "url = f\"{main_url}{query_url}{query}{page_url}{start}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req = requests.get(url)\n",
    "# soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "# # ul lists\n",
    "# ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "# # news links\n",
    "# links = ul_lists.find_all(\"a\", {\"class\": \"news_tit\"})\n",
    "# link_title_dict = {row[\"href\"]: {\"title\": row[\"title\"]} for row in links}\n",
    "\n",
    "\n",
    "# # update news texts\n",
    "# news_texts = ul_lists.find_all(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"})\n",
    "# for row in news_texts:\n",
    "#     link_title_dict[row[\"href\"]][\"text\"] = row.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request and soup\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "# ul lists and li lists\n",
    "ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "li_lists = ul_lists.findChildren(\"li\", {\"class\": \"bx\"})\n",
    "\n",
    "news_data = []\n",
    "for li in li_lists:\n",
    "\n",
    "    # news link & title\n",
    "    link = li.find(\"a\", {\"class\": \"news_tit\"})\n",
    "    href, title = link[\"href\"], link[\"title\"]\n",
    "\n",
    "    # news text\n",
    "    text = li.find(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"}).text\n",
    "\n",
    "    # thumb nail if exist\n",
    "    thumb = \"\"\n",
    "    thumb_link = li.find(\"img\", {\"class\": \"thumb api_get\"})\n",
    "    if thumb_link:\n",
    "        thumb = thumb_link[\"src\"]\n",
    "\n",
    "    # update link_title_dict\n",
    "    news_data.append({\"href\": href, \"title\": title, \"text\": text, \"thumb\": thumb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = li_lists[3]\n",
    "# li.find_all(\"a\", {\"class\": \"info\"})\n",
    "# # li.find(\"span\", {\"class\": \"info\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = li_lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언론사\n",
    "press = li.find(\"a\", {\"class\": \"info press\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 링크\n",
    "links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "naver_news_url = [link for link in links if link.text == \"네이버뉴스\"]\n",
    "if naver_news_url:\n",
    "    naver_news_url = naver_news_url[0]\n",
    "    naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "else:\n",
    "    naver_news_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=015&aid=0004675733'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naver_news_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Article(naver_news_url, language=\"ko\")\n",
    "a.download()\n",
    "a.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(a.html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"t11\">2022.03.20. 오전 8:02</span>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"span\", {\"class\": \"t11\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "# naver_news_url = [link for link in links if link.text == \"네이버뉴스\"][0]\n",
    "# naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "\n",
    "# # 언어가 한국어이므로 language='ko'로 설정\n",
    "# a = Article(naver_news_url, language=\"ko\")\n",
    "# a.download()\n",
    "# a.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for news in news_data:\n",
    "#     href = news[\"href\"]\n",
    "\n",
    "#     # 언어가 한국어이므로 language='ko'로 설정\n",
    "#     a = Article(href, language=\"ko\")\n",
    "#     a.download()\n",
    "#     a.parse()\n",
    "\n",
    "#     news[\"date\"] = a.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryNewsCrawler:\n",
    "    def __init__(self):\n",
    "        # 네이버 뉴스의 경우 page가 아닌\n",
    "        # 뉴스 기사 개수 카운팅으로 보여주고 있음\n",
    "        # 1 ~ 4000개까지 10개 단위로 보여줌\n",
    "        self.main_url = \"https://search.naver.com/search.naver\"\n",
    "        self.query_url = \"?where=news&sm=tab_pge&query=\"\n",
    "        self.page_url = \"&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=23&mynews=0&office_type=0&\"\n",
    "        self.page_url += (\n",
    "            \"office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=\"\n",
    "        )\n",
    "\n",
    "    def crawl_news_by_query(self, query: str, count: int = 200) -> List[Dict]:\n",
    "        self.query = query\n",
    "        parsed_query = urllib.parse.quote(query)\n",
    "\n",
    "        if count:\n",
    "            start_range = list(range(1, count, 10))\n",
    "        else:\n",
    "            start_range = list(range(1, 4000, 10))\n",
    "\n",
    "        news_data = []\n",
    "        for s_idx in tqdm(start_range):\n",
    "            url = f\"{self.main_url}{self.query_url}{parsed_query}{self.page_url}{s_idx}\"\n",
    "\n",
    "            # request and soup\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "            # ul lists and li lists\n",
    "            ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "            li_lists = ul_lists.findChildren(\"li\", {\"class\": \"bx\"})\n",
    "            for li in li_lists:\n",
    "                # news link & title\n",
    "                link = li.find(\"a\", {\"class\": \"news_tit\"})\n",
    "                href, title = link[\"href\"], link[\"title\"]\n",
    "                \n",
    "                # 언론사\n",
    "                press = li.find(\"a\", {\"class\": \"info press\"}).text\n",
    "                \n",
    "                # publish date\n",
    "                publish_date = self._get_datetime(href, li)\n",
    "\n",
    "                # news short_text\n",
    "                short_text = li.find(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"}).text\n",
    "                # thumb nail if exist\n",
    "                thumb = \"\"\n",
    "                thumb_link = li.find(\"img\", {\"class\": \"thumb api_get\"})\n",
    "                if thumb_link:\n",
    "                    thumb = thumb_link[\"src\"]\n",
    "\n",
    "                # update link_title_dict\n",
    "                news_data.append(\n",
    "                    {\n",
    "                        \"href\": href,\n",
    "                        \"date\": publish_date,\n",
    "                        'press': press,\n",
    "                        \"title\": title,\n",
    "                        \"short_text\": short_text,\n",
    "                        \"thumb\": thumb,\n",
    "                    }\n",
    "                )\n",
    "            time.sleep(random.uniform(0.6, 0.9))\n",
    "        return news_data\n",
    "\n",
    "    def _get_article(self, href: str, li: bs4.element.Tag) -> str:\n",
    "        links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "        naver_news_url = [link for link in links if link.text == \"네이버뉴스\"]\n",
    "        if naver_news_url:\n",
    "            naver_news_url = naver_news_url[0]\n",
    "            naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "        else:\n",
    "            naver_news_url = None\n",
    "        \n",
    "        if naver_news_url:\n",
    "            pass\n",
    "\n",
    "    def _get_datetime(self, href: str, li: bs4.element.Tag) -> datetime:\n",
    "        try:\n",
    "            a = Article(href, language=\"ko\")\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            publish_date = a.publish_date\n",
    "        except:\n",
    "            publish_date = None\n",
    "\n",
    "        if publish_date is None:\n",
    "            date = li.find(\"span\", {\"class\": \"info\"}).text\n",
    "            date = date.split()[0]\n",
    "\n",
    "            if \"분\" in date:\n",
    "                minutes = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(minutes=int(minutes))\n",
    "            elif \"시간\" in date:\n",
    "                hours = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(hours=int(hours))\n",
    "            elif \"일\" in date:\n",
    "                days = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(days=int(days))\n",
    "            else:\n",
    "                publish_date = datetime.strptime(date, \"%Y.%m.%d.\")\n",
    "        return publish_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = QueryNewsCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "query = \"코스피\"\n",
    "count = 10\n",
    "news_data = crawler.crawl_news_by_query(query=query, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news_data[0][\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca37adc465479a74c6a141ab03dbdf7641081253eef51a46dbf42999bc11eb1a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pt-py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
