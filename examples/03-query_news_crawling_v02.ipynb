{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query News Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import dill\n",
    "import requests\n",
    "import urllib\n",
    "import bs4\n",
    "import newspaper\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from kss import split_sentences\n",
    "\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm, trange\n",
    "from crawler.utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. QueryNewsCrawler class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 line by line coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"코스피\"\n",
    "# query = urllib.parse.quote(query)\n",
    "\n",
    "# main_url = \"https://search.naver.com/search.naver\"\n",
    "# query_url = \"?where=news&sm=tab_pge&query=\"\n",
    "# # 네이버 뉴스의 경우 page가 아닌\n",
    "# # 뉴스 기사 개수 카운팅으로 보여주고 있음\n",
    "# # 1 ~ 4000개까지 10개 단위로 보여줌\n",
    "# page_url = \"&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=23&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=\"\n",
    "\n",
    "# start = 1\n",
    "# start_range = list(range(1, 4000, 10))\n",
    "# url = f\"{main_url}{query_url}{query}{page_url}{start}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req = requests.get(url)\n",
    "# soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "# # ul lists\n",
    "# ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "# # news links\n",
    "# links = ul_lists.find_all(\"a\", {\"class\": \"news_tit\"})\n",
    "# link_title_dict = {row[\"href\"]: {\"title\": row[\"title\"]} for row in links}\n",
    "\n",
    "\n",
    "# # update news texts\n",
    "# news_texts = ul_lists.find_all(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"})\n",
    "# for row in news_texts:\n",
    "#     link_title_dict[row[\"href\"]][\"text\"] = row.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # request and soup\n",
    "# req = requests.get(url)\n",
    "# soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "# # ul lists and li lists\n",
    "# ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "# li_lists = ul_lists.findChildren(\"li\", {\"class\": \"bx\"})\n",
    "\n",
    "# news_data = []\n",
    "# for li in li_lists:\n",
    "\n",
    "#     # news link & title\n",
    "#     link = li.find(\"a\", {\"class\": \"news_tit\"})\n",
    "#     href, title = link[\"href\"], link[\"title\"]\n",
    "\n",
    "#     # news text\n",
    "#     text = li.find(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"}).text\n",
    "\n",
    "#     # thumb nail if exist\n",
    "#     thumb = \"\"\n",
    "#     thumb_link = li.find(\"img\", {\"class\": \"thumb api_get\"})\n",
    "#     if thumb_link:\n",
    "#         thumb = thumb_link[\"src\"]\n",
    "\n",
    "#     # update link_title_dict\n",
    "#     news_data.append({\"href\": href, \"title\": title, \"text\": text, \"thumb\": thumb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = li_lists[3]\n",
    "# li.find_all(\"a\", {\"class\": \"info\"})\n",
    "# # li.find(\"span\", {\"class\": \"info\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 뉴스 링크\n",
    "# links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "# naver_news_url = [link for link in links if link.text == \"네이버뉴스\"]\n",
    "# if naver_news_url:\n",
    "#     naver_news_url = naver_news_url[0]\n",
    "#     naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "# else:\n",
    "#     naver_news_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "# naver_news_url = [link for link in links if link.text == \"네이버뉴스\"][0]\n",
    "# naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "\n",
    "# # 언어가 한국어이므로 language='ko'로 설정\n",
    "# a = Article(naver_news_url, language=\"ko\")\n",
    "# a.download()\n",
    "# a.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for news in news_data:\n",
    "#     href = news[\"href\"]\n",
    "\n",
    "#     # 언어가 한국어이므로 language='ko'로 설정\n",
    "#     a = Article(href, language=\"ko\")\n",
    "#     a.download()\n",
    "#     a.parse()\n",
    "\n",
    "#     news[\"date\"] = a.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryNewsCrawler:\n",
    "    def __init__(self):\n",
    "        # 네이버 뉴스의 경우 page가 아닌\n",
    "        # 뉴스 기사 개수 카운팅으로 보여주고 있음\n",
    "        # 1 ~ 4000개까지 10개 단위로 보여줌\n",
    "        self.main_url = \"https://search.naver.com/search.naver\"\n",
    "        self.query_url = \"?where=news&sm=tab_pge&query=\"\n",
    "        self.page_url = \"&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=23&mynews=0&office_type=0&\"\n",
    "        self.page_url += (\n",
    "            \"office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=\"\n",
    "        )\n",
    "\n",
    "    def crawl_news_by_query(self, query: str, count: int = 200) -> List[Dict]:\n",
    "        self.query = query\n",
    "        parsed_query = urllib.parse.quote(query)\n",
    "\n",
    "        if count:\n",
    "            start_range = list(range(1, count, 10))\n",
    "        else:\n",
    "            start_range = list(range(1, 4000, 10))\n",
    "\n",
    "        news_data = []\n",
    "        for s_idx in tqdm(start_range):\n",
    "            url = f\"{self.main_url}{self.query_url}{parsed_query}{self.page_url}{s_idx}\"\n",
    "\n",
    "            # request and soup\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "            # ul lists and li lists\n",
    "            ul_lists = soup.find(\"ul\", {\"class\": \"list_news\"})\n",
    "            li_lists = ul_lists.findChildren(\"li\", {\"class\": \"bx\"})\n",
    "            for li in li_lists:\n",
    "                # news link & title\n",
    "                link = li.find(\"a\", {\"class\": \"news_tit\"})\n",
    "                href, title = link[\"href\"], link[\"title\"]\n",
    "\n",
    "                # newspaper3k instance\n",
    "                try:\n",
    "                    news_instance = Article(href, language=\"ko\")\n",
    "                    news_instance.download()\n",
    "                    news_instance.parse()\n",
    "                except:\n",
    "                    news_instance = None\n",
    "\n",
    "                # 언론사\n",
    "                press = li.find(\"a\", {\"class\": \"info press\"}).text\n",
    "\n",
    "                # publish date\n",
    "                publish_date = self._get_datetime(news_instance, li)\n",
    "\n",
    "                # news short_text\n",
    "                short_text = li.find(\"a\", {\"class\": \"api_txt_lines dsc_txt_wrap\"}).text\n",
    "\n",
    "                # news text and top_image\n",
    "                text, top_iamge = self._get_article(news_instance, li)\n",
    "\n",
    "                # thumb nail if exist\n",
    "                thumb = \"\"\n",
    "                thumb_link = li.find(\"img\", {\"class\": \"thumb api_get\"})\n",
    "                if thumb_link:\n",
    "                    thumb = thumb_link[\"src\"]\n",
    "\n",
    "                # update link_title_dict\n",
    "                news_data.append(\n",
    "                    {\n",
    "                        \"href\": href,\n",
    "                        \"date\": publish_date,\n",
    "                        \"press\": press,\n",
    "                        \"title\": title,\n",
    "                        \"short_text\": short_text,\n",
    "                        \"text\": text,\n",
    "                        \"top_image\": top_iamge,\n",
    "                        \"thumb\": thumb,\n",
    "                    }\n",
    "                )\n",
    "            time.sleep(random.uniform(0.6, 0.9))\n",
    "        return news_data\n",
    "\n",
    "    def _get_article(\n",
    "        self, article: newspaper.article.Article, li: bs4.element.Tag\n",
    "    ) -> str:\n",
    "        text, top_image = None, None\n",
    "        if article is not None:\n",
    "            text = article.text\n",
    "            top_img = article.top_image\n",
    "\n",
    "        links = li.find_all(\"a\", {\"class\": \"info\"})\n",
    "        naver_news_url = [link for link in links if link.text == \"네이버뉴스\"]\n",
    "        if naver_news_url:\n",
    "            naver_news_url = naver_news_url[0]\n",
    "            naver_news_url = naver_news_url[\"href\"].replace(\"&amp;\", \"&\")\n",
    "        else:\n",
    "            naver_news_url = None\n",
    "\n",
    "        if text is None and naver_news_url is not None:\n",
    "            try:\n",
    "                a = Article(naver_news_url, language=\"ko\")\n",
    "                a.download()\n",
    "                a.parse()\n",
    "\n",
    "                text = a.text\n",
    "            except:\n",
    "                pass\n",
    "        elif top_image is None and naver_news_url is not None:\n",
    "            try:\n",
    "                a = Article(naver_news_url, language=\"ko\")\n",
    "                a.download()\n",
    "                a.parse()\n",
    "\n",
    "                top_image = a.top_image\n",
    "            except:\n",
    "                pass\n",
    "        return text, top_image\n",
    "\n",
    "    def _get_datetime(\n",
    "        self,\n",
    "        article: newspaper.article.Article,\n",
    "        li: bs4.element.Tag,\n",
    "    ) -> datetime:\n",
    "        publish_date = None\n",
    "        if article is not None:\n",
    "            publish_date = article.publish_date\n",
    "\n",
    "        if publish_date is None:\n",
    "            date_list = li.find_all(\"span\", {\"class\": \"info\"})\n",
    "            date = None\n",
    "            if len(date_list) > 1:\n",
    "                date = date_list[-1].text\n",
    "            else:\n",
    "                date = li.find(\"span\", {\"class\": \"info\"}).text\n",
    "                date = date.split()[0]\n",
    "\n",
    "            if \"분\" in date:\n",
    "                minutes = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(minutes=int(minutes))\n",
    "            elif \"시간\" in date:\n",
    "                hours = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(hours=int(hours))\n",
    "            elif \"일\" in date:\n",
    "                days = re.sub(r\"[^\\d+]\", \"\", date)\n",
    "                publish_date = datetime.now() - timedelta(days=int(days))\n",
    "            else:\n",
    "                try:\n",
    "                    publish_date = datetime.strptime(date, \"%Y.%m.%d.\")\n",
    "                except:\n",
    "                    pass\n",
    "        return publish_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = QueryNewsCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.68s/it]\n"
     ]
    }
   ],
   "source": [
    "query = \"코스피\"\n",
    "count = 10\n",
    "news_data = crawler.crawl_news_by_query(query=query, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': 'https://www.hankyung.com/finance/article/2022031963016',\n",
       " 'date': datetime.datetime(2022, 3, 20, 8, 0, 5, tzinfo=tzoffset(None, 32400)),\n",
       " 'press': '한국경제언론사 선정',\n",
       " 'title': '악재에 내성 생긴 코스피…\"낙폭과대·내수株 주목\" [주간전망]',\n",
       " 'short_text': '코스피가 반등을 모색하고 있다. 아직 증시를 둘러싼 악재의 해소 국면으로 접어들었다고 보기는 어렵지만... 20일 한국거래소에 따르면 지난주(14~18일) 코스피는 1.72% 오른 2707.02에 거래를 마쳤다. 이 기간 동안...',\n",
       " 'text': '글로벌증시, 유가 100달러 재돌파에도 강세 유지\\n\\nNH투자증권, 예상밴드로 2650~2800 제시\\n\\n“업종·종목별 순환매 가능성”\\n\\n사진=연합뉴스\\n\\n코스피가 반등을 모색하고 있다. 아직 증시를 둘러싼 악재의 해소 국면으로 접어들었다고 보기는 어렵지만, 증시가 우크라이나 전쟁, 원자재 가격 급등, 통화 긴축에 따른 경기 위축 등 우려스러운 이슈에 대한 내성을 확보한 모습이다.증권가도 증시의 하방 압력보다는 상승 여력에 주목할 때라는 분석에 힘을 실으며, 펀더멘털에 주목하라고 조언한다.20일 한국거래소에 따르면 지난주(14~18일) 코스피는 1.72% 오른 2707.02에 거래를 마쳤다.이 기간 동안 외국인이 유가증권시장에서 1조896억원 어치의 주식을 팔았다. 일별로는 지난 17일 하루를 제외하고 모두 순매도했다. 외국인의 매도세는 지난 4일 이후 8거래일동안 이어졌고, 하루 순매수한 뒤 18일에는 다시 1030억원 어치를 팔았다.기관과 개인은 각각 6549억원 어치와 4272억원 어치를 순매수했다.주초에는 중국에서 코로나19 오미크론 변이의 확산으로 4대 1선도시 중 하나인 선전시를 일주일동안 봉쇄하기로 했다는 소식이 증시를 짓눌렀다.반등의 계기는 국제유가의 급락이었다. 뉴욕상업거래소에서 지난 8일(현지시간) 배럴당 123.70달러까지 치솟았던 서부텍사스산원유(WTI)는 14~15일 이틀 연속 급락하며 100달러선 아래로 내려갔다.한번 반등하기 시작하자 시장은 다양한 이슈에 낙관적인 평가를 했다.미국 중앙은행(연방준비제도·Fed)은 지난 15~16일 개최한 3월 연방공개시장위원회(FOMC) 정례회의를 마친 뒤 △기준금리를 25bp(베이시스포인트·0.01%포인트) 인상하고 △올해 총 7번의 금리 인상을 예고하는 점도표는 내놓는 한편 △오는 5월부터 시장에 채권을 팔며 유동성을 거둬들이는 양적 긴축에 나설 가능성을 언급했다.공격적인 통화 긴축을 예고한 매파(통화긴축 선호론자)적인 결과였지만, 시장은 제롬 파월 연준 의장의 경기에 대한 자신감에 주목했다. 또 양적긴축에 대한 언급을 ‘불확실성 해소’로 받아들였다.서정훈 삼성증권 연구원은 “주식시장의 추세 반전을 위해서는 인플레이션 압력 완화가 급선무라는 게 분명하다는 측면에서 연준의 긴축 행보가 필요악이 될 수 있다”며 “긴축은 유동성 환경에 부담이 될 것이나, 드높아진 인플레이션 전망을 낮추는 데 불가결한 요소”라고 말했다.국제유가의 반등도 증시 상승세를 꺾지 못했다. 17일(현지시간) WTI가 다시 배럴당 100달러선을 넘어섰지만, 18일(한국시간) 코스피는 상승세를 이어가며 2700선을 돌파했다.뒤이어 개장한 18일 미국 시장에서는 WTI가 추가로 상승해 배럴당 103.09달러까지 올랐지만, 뉴욕증시는 나스닥이 2% 넘게 상승할 정도로 강한 모습을 보였다.김영환 NH투자증권 연구원은 “최근 글로벌 주식 시장은 악재에 대한 내성이 높아진 모습”이라며 “3월 FOMC의 기준금리 인상 및 연내 추가 6회의 인상 예고, 국재유가의 배럴당 100달러 재돌파 등의 악재에도 불구하고 주식시장이 반등 흐름을 보이는 건 그만큼 악재를 많이 반영하고 있었음을 방증한다”고 말했다.NH투자증권은 주간 코스피의 예상 밴드로 2650~2800을 제시했다.다만 반등이 강하게 나타나기는 힘들다는 공감대도 증권가에 형성돼 있다.김일혁 KB증권 연구원은 “최근 시장 반등은 기술적 요인들이 영향을 미쳤다”며 “(악재들에 대한) 극단적인 시나리오를 반영해서 하락한 폭만큼은 되돌려지겠지만, 최근 반등이 다소 낙관적인 기대에 반응한 측면도 없지 않다. 경기에 대한 우려가 낮아지면서 진행된 반등은 반납할 가능성이 높다”고 분석했다.신승진 삼성증권 연구원도 “급격한 인플레이션에 따른 영향이 반영된 1분기 실적을 확인하려는 투자자들의 관망심리가 클 것”이라며 “시장이 박스권의 움직임을 보인다면, 업종·종목별 순환매가 지속될 가능성이 높다”고 말했다. 그러면서 투자 아이디어로 △낙폭과대 종목 △리오프닝 및 정책 변화 수혜 종목 △글로벌 공급망 재편 수혜 종목을 제시했다.김영환 연구원은 “업종 관점에서 대외 리스크에 대한 노출도가 높은 종목들은 피해갈 필요가 있다”며 “원재료 가격 상승에 따른 비용증가분을 제품 가격에 반영하기 용이한 업종, 대외 리스크와 연관이 적은 국내 내수 소비 업종, 낙폭과대 성장주에 관심을 두라”고 조언했다.한경우 한경닷컴 기자 case@hankyung.com',\n",
       " 'top_image': 'https://imgnews.pstatic.net/image/015/2022/03/20/0004675733_001_20220320080203318.jpg',\n",
       " 'thumb': 'https://search.pstatic.net/common/?src=https%3A%2F%2Fimgnews.pstatic.net%2Fimage%2Forigin%2F015%2F2022%2F03%2F20%2F4675733.jpg&type=ff264_180&expire=2&refresh=true'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca37adc465479a74c6a141ab03dbdf7641081253eef51a46dbf42999bc11eb1a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pt-py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
